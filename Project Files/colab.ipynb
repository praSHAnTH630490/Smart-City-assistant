{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a1835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pkill -f ngrok\n",
    "# âœ… STEP 1: Install dependencies\n",
    "!pip install -q transformers accelerate einops flask flask-cors pyngrok\n",
    "\n",
    "# âœ… STEP 2: Ngrok setup\n",
    "from pyngrok import ngrok\n",
    "import time, threading, requests\n",
    "\n",
    "# Replace this with your own ngrok token\n",
    "!ngrok config add-authtoken 2yiAfHLrsyI8hLMQuYXBUGagG5M_af7BCRMVXZ9y5uMjTuXX\n",
    "\n",
    "# Open tunnel to port 5000\n",
    "public_tunnel = ngrok.connect(5000)\n",
    "print(\"ðŸš€ Public URL:\", public_tunnel.public_url)\n",
    "\n",
    "# Save URL for backend/frontend access\n",
    "with open(\"/content/granite_url.txt\", \"w\") as f:\n",
    "    f.write(public_tunnel.public_url + \"/chat\")\n",
    "\n",
    "# âœ… STEP 3: Load IBM Granite Model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "HF_TOKEN = \"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\"  # ðŸ”’ Replace with your actual Hugging Face token\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HF_TOKEN,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ").eval()\n",
    "\n",
    "# âœ… STEP 4: Define model interaction\n",
    "def ask_granite(prompt: str) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# âœ… STEP 5: Define Flask app\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.get_json(silent=True) or {}\n",
    "    prompt = data.get(\"prompt\", \"\").strip()\n",
    "\n",
    "    if not prompt:\n",
    "        return jsonify({\"error\": \"Missing 'prompt'\"}), 400\n",
    "\n",
    "    try:\n",
    "        response = ask_granite(prompt)\n",
    "        return jsonify({\"text\": response})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Model call failed: {e}\"}), 500\n",
    "\n",
    "# âœ… STEP 6: Keep ngrok alive\n",
    "def keep_alive():\n",
    "    def ping():\n",
    "        while True:\n",
    "            try:\n",
    "                requests.get(public_tunnel.public_url + \"/chat\")\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(600)  # Ping every 10 minutes\n",
    "    threading.Thread(target=ping, daemon=True).start()\n",
    "\n",
    "keep_alive()\n",
    "\n",
    "# âœ… STEP 7: Run Flask app (this blocks the cell)\n",
    "app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0037e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e104e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
